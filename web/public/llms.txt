# md.succ.ai

> URL to Markdown API. Convert any webpage, document, or YouTube video to readable Markdown optimized for AI consumption.

## What it does

md.succ.ai converts any URL to clean Markdown using a 9-pass extraction pipeline (Readability, Defuddle, article-extractor, CSS selectors, Schema.org, Open Graph, text density, and more). It strips navigation, sidebars, footers, and ads - returning only the article content. YouTube URLs return transcripts with timestamps.

## API Usage

No API key required. No authentication. Rate limits: 60 req/min (convert), 10 req/min (extract), 5 req/min (batch).

### Endpoints

- GET https://md.succ.ai/{url} - convert URL to Markdown
- GET https://md.succ.ai/?url={url} - query param format
- POST https://md.succ.ai/batch - batch convert multiple URLs
- POST https://md.succ.ai/extract - structured data extraction (JSON schema)
- GET https://md.succ.ai/health - health check

### Query Parameters

- ?links=citations - convert inline links to numbered references with footer
- ?mode=fit - prune boilerplate for LLM-optimized output
- ?max_tokens=N - truncate output to N tokens

### Examples

```
# Basic conversion
curl https://md.succ.ai/https://example.com

# YouTube transcript
curl https://md.succ.ai/https://youtube.com/watch?v=dQw4w9WgXcQ

# Citation-style links
curl "https://md.succ.ai/?url=https://en.wikipedia.org/wiki/Markdown&links=citations"

# LLM-optimized
curl "https://md.succ.ai/?url=https://htmx.org/docs/&mode=fit"

# Batch conversion (up to 50 URLs)
curl -X POST https://md.succ.ai/batch \
  -H "Content-Type: application/json" \
  -d '{"urls": ["https://example.com", "https://httpbin.org/html"], "options": {"mode": "fit"}}'

# Structured extraction
curl -X POST https://md.succ.ai/extract \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com", "schema": {"type": "object", "properties": {"title": {"type": "string"}}}}'
```

### JSON Response (Accept: application/json)

```json
{
  "title": "Example Domain",
  "url": "https://example.com",
  "content": "# Example Domain\n\nThis domain is for use in documentation...",
  "tokens": 33,
  "tier": "fetch",
  "method": "readability",
  "quality": { "score": 0.85, "grade": "A" },
  "time_ms": 245,
  "excerpt": "This domain is for use in...",
  "byline": "",
  "siteName": ""
}
```

### Batch Response

```json
{
  "results": [
    { "url": "https://example.com", "title": "Example Domain", "content": "...", "tokens": 33, "tier": "fetch", "quality": {"score": 0.85, "grade": "A"}, "time_ms": 245 },
    { "url": "https://httpbin.org/html", "title": "...", "content": "...", "tokens": 120, "tier": "fetch", "quality": {"score": 0.9, "grade": "A"}, "time_ms": 310 }
  ],
  "total": 2,
  "total_tokens": 153
}
```

### Response Headers

- x-markdown-tokens: cl100k_base token count
- x-conversion-tier: fetch | browser | llm | youtube | document:pdf
- x-conversion-time: conversion time in milliseconds
- x-extraction-method: readability | defuddle | article-extractor | pdf | ...
- x-quality-score: quality score 0-1
- x-quality-grade: quality grade A-F
- x-cache: hit | miss
- etag: content hash for conditional requests (use If-None-Match for 304)
- x-ratelimit-limit: max requests per minute
- x-ratelimit-remaining: requests remaining in current window
- x-ratelimit-reset: Unix timestamp when window resets

### Conditional Requests (ETag)

Responses include an ETag header. Send If-None-Match with the ETag value to receive a 304 Not Modified if content hasn't changed. Saves bandwidth on repeated requests.

```
# First request - get the ETag
curl -sI https://md.succ.ai/https://example.com
# -> etag: W/"a1b2c3d4..."

# Subsequent request - check if content changed
curl -H 'If-None-Match: W/"a1b2c3d4..."' https://md.succ.ai/https://example.com
# -> 304 Not Modified (no body, saves bandwidth)
```

## Pipeline

1. **YouTube detection**: YouTube URLs bypass HTML pipeline - transcript extracted via innertube API
2. **Document detection**: PDF, DOCX, XLSX, CSV detected by Content-Type or extension
3. **Tier 1 (fast)**: 9-pass extraction pipeline (200-500ms) with quality ratio checks
4. **Tier 2 (browser)**: Playwright headless Chromium for SPAs and JS-heavy sites (3-15s)
5. **Tier 2.5 (LLM)**: Llama 3.3 70B via nano-gpt when quality < B

Automatic tier escalation - no configuration needed.

## Caching

Responses are cached with tier-appropriate TTL:
- YouTube transcripts: 1 hour (content rarely changes)
- PDF/DOCX/XLSX: 2 hours (documents are immutable)
- Browser tier: 10 minutes
- Fetch tier: 5 minutes (default)

Cache-Control header reflects the actual TTL. Tracking params (utm_*, fbclid, gclid) are stripped from cache keys.

## Supported Formats

- HTML - 9-pass extraction + Turndown
- PDF - text extraction via unpdf
- DOCX - mammoth -> HTML -> Turndown
- XLSX/XLS - SheetJS -> Markdown tables
- CSV - SheetJS -> Markdown table
- YouTube - transcript with timestamps

## Self-Hosting

```bash
git clone https://github.com/vinaes/md-succ-ai.git
cd md-succ-ai
docker compose up -d
```

Available at localhost:3100. Docker image includes Chromium.

## Use Cases

- AI agent web browsing (MCP tools, function calling)
- RAG pipeline document ingestion
- Content extraction for LLM context windows
- YouTube transcript extraction for video summarization
- Structured data extraction from any webpage
- Batch processing for web research automation
- Documentation scraping

## Technology

- Node.js 22, Hono framework
- Mozilla Readability + Defuddle (content extraction)
- Turndown (HTML to Markdown)
- Playwright (headless Chromium for SPAs)
- Ajv (JSON Schema validation)
- gpt-tokenizer (cl100k_base token counting)

## Links

- Website: https://md.succ.ai
- GitHub: https://github.com/vinaes/md-succ-ai
- Part of the succ ecosystem: https://succ.ai
- License: FSL-1.1-Apache-2.0

## Contact

Vinaes Code Ltd
https://succ.ai
